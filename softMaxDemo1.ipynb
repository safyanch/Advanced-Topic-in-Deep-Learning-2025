{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc36c084-18d5-4df3-a70e-2c038fa4513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1fbfbec-1756-45c8-8c7e-458d48de32fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # shape: (150, 4)\n",
    "y = iris.target.reshape(-1, 1)  # shape: (150, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e42377b-121d-4072-adfa-c1c62388eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "Y = encoder.fit_transform(y)  # shape: (150, 3)\n",
    "\n",
    "# Normalize features\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize weights and bias\n",
    "n_features = X.shape[1]      # 4\n",
    "n_classes = Y.shape[1]       # 3\n",
    "W = np.random.randn(n_features, n_classes) * 0.01  # shape: (4, 3)\n",
    "b = np.zeros((1, n_classes))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7047747-4dd7-48d2-b56e-426e87d2b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Softmax function\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # for numerical stability\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c10ed374-3a43-407f-b6c0-d3e3b22b2a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "                   \n",
    "\n",
    "# Softmax function\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # for numerical stability\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75b09faf-46be-483e-a2a5-aae0be82741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    loss = -np.sum(y_true * np.log(y_pred + 1e-9)) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e35c80f-331c-4168-9a7f-380a6608ab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Accuracy function\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7f3cfa4-681a-440f-8bb1-9d2796d7e20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 1.0995, Accuracy = 0.2833\n",
      "Epoch 100: Loss = 0.3396, Accuracy = 0.9000\n",
      "Epoch 200: Loss = 0.2701, Accuracy = 0.9000\n",
      "Epoch 300: Loss = 0.2289, Accuracy = 0.9333\n",
      "Epoch 400: Loss = 0.2008, Accuracy = 0.9417\n",
      "Epoch 500: Loss = 0.1805, Accuracy = 0.9583\n",
      "Epoch 600: Loss = 0.1650, Accuracy = 0.9667\n",
      "Epoch 700: Loss = 0.1529, Accuracy = 0.9667\n",
      "Epoch 800: Loss = 0.1432, Accuracy = 0.9667\n",
      "Epoch 900: Loss = 0.1352, Accuracy = 0.9667\n",
      "\n",
      "Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training loop\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    z = np.dot(X_train, W) + b       # shape: (m, 3)\n",
    "    y_hat = softmax(z)               # shape: (m, 3)\n",
    "    \n",
    "    # Loss and accuracy\n",
    "    loss = cross_entropy(Y_train, y_hat)\n",
    "    acc = accuracy(Y_train, y_hat)\n",
    "    \n",
    "    # Backward pass\n",
    "    m = X_train.shape[0]\n",
    "    dz = (y_hat - Y_train) / m             # shape: (m, 3)\n",
    "    dW = np.dot(X_train.T, dz)             # shape: (4, 3)\n",
    "    db = np.sum(dz, axis=0, keepdims=True) # shape: (1, 3)\n",
    "    \n",
    "    # Update weights\n",
    "    W -= learning_rate * dW\n",
    "    b -= learning_rate * db\n",
    "\n",
    "    # Print progress\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss:.4f}, Accuracy = {acc:.4f}\")\n",
    "\n",
    "# Test evaluation\n",
    "z_test = np.dot(X_test, W) + b\n",
    "y_test_pred = softmax(z_test)\n",
    "test_acc = accuracy(Y_test, y_test_pred)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5db0dcb-50ca-4da8-a4f2-4d9195fe6d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
